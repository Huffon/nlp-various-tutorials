{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.prune as prune\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.4.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2SeqAttention(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(2000, 200)\n",
      "    (lstm): LSTM(200, 200, bidirectional=True)\n",
      "    (fc): Linear(in_features=400, out_features=200, bias=True)\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (attention): Attention(\n",
      "    (attention): Linear(in_features=600, out_features=200, bias=True)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (attention): Attention(\n",
      "      (attention): Linear(in_features=600, out_features=200, bias=True)\n",
      "    )\n",
      "    (embedding): Embedding(3000, 200)\n",
      "    (lstm): LSTM(600, 200)\n",
      "    (fc): Linear(in_features=800, out_features=3000, bias=True)\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, \n",
    "                       embed_dim, \n",
    "                       enc_hidden_dim, \n",
    "                       dec_hidden_dim, \n",
    "                       dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim,\n",
    "                           enc_hidden_dim,\n",
    "                           bidirectional=True)\n",
    "        self.fc = nn.Linear(enc_hidden_dim * 2, dec_hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, source, source_len):\n",
    "        '''\n",
    "        source = [batch size, source length]\n",
    "        '''\n",
    "        embedded = self.dropout(self.embedding(source))\n",
    "        # embedded  = [batch size, source length, embed dim]\n",
    "\n",
    "        packed_embedded = pack_padded_sequence(embedded, source_len)\n",
    "        packed_output, (hidden, _) = self.lstm(packed_embedded)      \n",
    "        output, _ = pad_packed_sequence(packed_output)\n",
    "        # output    = [batch size, source length, enc hidden dim * 2]\n",
    "        # hidden    = [batch size, num layers * 2, enc hidden dim]\n",
    "        # [forward_1, backward_1, forward_2, backward_2, ..., forward_n, backward_n]\n",
    "        \n",
    "        hidden = torch.tanh(self.fc(torch.cat((hidden[:, -2, :], hidden[:, -1, :]), dim=1)))\n",
    "        return output, hidden\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, enc_hidden_dim, \n",
    "                       dec_hidden_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attention = nn.Linear(dec_hidden_dim + (enc_hidden_dim * 2), dec_hidden_dim)\n",
    "        self.weight = self.attention.weight\n",
    "        self.bias = self.attention.bias\n",
    "        self.v = nn.Parameter(torch.rand(dec_hidden_dim))\n",
    "\n",
    "    def forward(self, hidden, encoder_output, mask):\n",
    "        '''\n",
    "        디코더의 이전 'hidden state'와 인코더의 output을 입력 값으로 받아\n",
    "        hidden state가 인코더의 어느 부분을 참조할지를 계산\n",
    "        \n",
    "        hidden         = [batch size, dec hidden dim]\n",
    "        encoder_output = [batch size, source length, enc hidden dim * 2]\n",
    "        mask           = [batch size, source length]\n",
    "        '''\n",
    "        hidden = hidden.unsqueeze(1)  \n",
    "        batch_size, src_len = encoder_output.size()\n",
    "        # hidden         = [batch size, 1, dec hidden dim]\n",
    "\n",
    "        # 인코더의 output과 길이를 맞추기 위해 hidden을 src_len 만큼 반복\n",
    "        hidden = hidden.repeat(1, src_len, 1)\n",
    "        # hidden         = [batch size, source length, dec hidden dim]\n",
    "\n",
    "        # 디코더의 hidden과 인코더의 output 내 토큰들이 얼마나 매치되는지 계산\n",
    "        energy = torch.tanh(self.attention(torch.cat((hidden, encoder_output), dim=2)))\n",
    "        # energy         = [batch size, source length, dec hidden dim]\n",
    "\n",
    "        energy = energy.permute(0, 2, 1)\n",
    "        # energy         = [batch size, dec hidden dim, source length]\n",
    "\n",
    "        # 이제 energy를 사용해 어텐션 맵을 만들기 위해 [batch size, source length]로 줄여주어야 함\n",
    "        # 이는 [batch size, 1, dec hidden dim] 사이즈의 텐서 \"v\"를 활용해 달성 가능\n",
    "        v = self.v.repeat(batch_size, 1).unsqueeze(1)\n",
    "        # v = [batch size, 1, dec hidden dim]\n",
    "\n",
    "        # 이제 dec hidden dim 차원 내 원소들에 가중합을 적용\n",
    "        # cf. 이때, \"v\"는 학습 가능한 가중치이기 때문에 우리는 이를 'Parameterized Attention' 이라 부름\n",
    "\n",
    "        # bmm is a batch matrix-matrix product: [batch size, a, b] * [batch size, b, c] = [batch size, a, c]\n",
    "        attention = torch.bmm(v, energy).squeeze(1)\n",
    "        # attention = [batch size, source length]\n",
    "\n",
    "        # 마스크를 이용해 패드 토큰에 어텐션을 주지 않도록 설정\n",
    "        attention = attention.masked_fill(mask == 1, -1e10)\n",
    "\n",
    "        return F.softmax(attention, dim=1)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_dim, \n",
    "                       embed_dim, \n",
    "                       enc_hidden_dim, \n",
    "                       dec_hidden_dim,\n",
    "                       output_dim,\n",
    "                       dropout,\n",
    "                       attention\n",
    "                ):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.attention = attention\n",
    "        self.embedding = nn.Embedding(output_dim, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim + (enc_hidden_dim * 2), dec_hidden_dim)\n",
    "        self.fc = nn.Linear(dec_hidden_dim + embed_dim + (enc_hidden_dim * 2), output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, target, hidden, encoder_output, mask):\n",
    "        '''\n",
    "        target         = [batch size]\n",
    "        hidden         = [batch size, dec hidden dim]\n",
    "        encoder_output = [batch size, batch size, enc hidden dim * 2]\n",
    "        mask           = [batch size, source length]\n",
    "        '''\n",
    "        target = target.unsqueeze(0)  \n",
    "        # target = [batch size, 1]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(target))\n",
    "        # embedded = [batch size, 1, embed dim]\n",
    "\n",
    "        # 이전 hidden state와 인코더 output을 활용해 어텐션 벡터 계산\n",
    "        attention = self.attention(hidden, encoder_output, mask).unsqueeze(1)\n",
    "        # attention = [batch size, 1, source length]\n",
    "\n",
    "        # 어텐션과 인코더 output을 이용해 가중합의 컨텍스트 벡터 생성\n",
    "        weighted = torch.bmm(attention, encoder_output)\n",
    "        # weighted = [batch size, 1, enc hidden dim * 2]\n",
    "\n",
    "        dec_input = torch.cat((embedded, weighted), dim=2)\n",
    "        # dec_input = [batch size, 1, embed dim + (enc hidden dim * 2)]\n",
    "\n",
    "        output, (hidden, _) = self.lstm(dec_input, hidden.unsqueeze(0))\n",
    "        # output = [batch size, 1, dec hidden dim]\n",
    "        # hidden = [batch size, 1, dec hidden dim]\n",
    "\n",
    "        assert (output == hidden).all()\n",
    "\n",
    "        output = output.squeeze(1)      # [batch size, dec hidden dim]    : 디코더의 현재 히든 스테이트\n",
    "        embedded = embedded.squeeze(1)  # [batch size, embed dim]         : 디코더의 현재 타겟 토큰\n",
    "        weighted = weighted.squeeze(1)  # [batch size, enc hidden dim * 2]: 어텐션을 이용한 가중합\n",
    "\n",
    "        pred = self.fc(torch.cat((output, weighted, embedded), dim=1))\n",
    "        # pred = [batch size, output dim]\n",
    "\n",
    "        # return a prediction, a new hidden state and attention tensor\n",
    "        return prediction, hidden.squeeze(1), attention.squeeze(1)\n",
    "\n",
    "\n",
    "class Seq2SeqAttention(nn.Module):\n",
    "    def __init__(self, input_dim, \n",
    "                       embed_dim, \n",
    "                       enc_hidden_dim, \n",
    "                       dec_hidden_dim, \n",
    "                       dropout,\n",
    "                       output_dim, \n",
    "                       pad_idx):\n",
    "        super(Seq2SeqAttention, self).__init__()\n",
    "        self.encoder = Encoder(\n",
    "            input_dim,\n",
    "            embed_dim, \n",
    "            enc_hidden_dim, \n",
    "            dec_hidden_dim, \n",
    "            dropout\n",
    "        )\n",
    "        self.attention = Attention(\n",
    "            enc_hidden_dim, \n",
    "            dec_hidden_dim\n",
    "        )\n",
    "        self.decoder = Decoder(\n",
    "            input_dim, \n",
    "            embed_dim, \n",
    "            enc_hidden_dim, \n",
    "            dec_hidden_dim,\n",
    "            output_dim,\n",
    "            dropout,\n",
    "            self.attention\n",
    "        )\n",
    "        self.pad_idx = pad_idx\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def create_mask(self, source):\n",
    "        mask = (source == self.pad_idx)\n",
    "        return mask\n",
    "\n",
    "    def forward(self, source, source_length, target, teacher_forcing=0.5):\n",
    "        '''\n",
    "        source = [batch size, source length]\n",
    "        target = [batch size, target length]\n",
    "        '''\n",
    "\n",
    "        target_max_len, batch_size = target.size()\n",
    "        outputs = torch.zeros(batch_size, target_max_len, self.output_dim).to(self.device)\n",
    "        # outputs = [batch size, target length, output dim]\n",
    "\n",
    "        # 어텐션을 저장할 텐서 정의\n",
    "        attentions = torch.zeros(batch_size, target_max_len, source.shape[0]).to(self.device)\n",
    "        # attentions = [batch size, target length, source length]\n",
    "\n",
    "        encoder_output, hidden = self.encoder(source, source_length)\n",
    "        # encoder_output    = [batch size, source length, enc hidden dim * 2]\n",
    "        # hidden            = [batch size, dec hidden dim]\n",
    "\n",
    "        # 디코더의 초기 입력 값은 <SOS> 토큰\n",
    "        dec_input = target[0, :]\n",
    "        # input = [batch size]\n",
    "\n",
    "        mask = self.create_mask(source)\n",
    "        # mask = [batch size, source length]\n",
    "\n",
    "        for t in range(1, target_max_len):\n",
    "            output, hidden, attention = self.decoder(dec_input, hidden, encoder_output, mask)\n",
    "            # output    = [batch size, output dim]\n",
    "            # hidden    = [batch size, dec hidden dim]\n",
    "            # attention = [batch size, source length]\n",
    "\n",
    "            # 앞서 정의한 outputs와 attnetions를 해당 타입 스텝에 맞게 채워나감\n",
    "            outputs[t] = output\n",
    "            attentions[t] = attention\n",
    "\n",
    "            # Teacher forcing 확률 계산\n",
    "            teacher_force = random.random() < teacher_forcing\n",
    "\n",
    "            # 가장 높은 확률로 다음에 올 토큰 예측 \n",
    "            # output.max(1)는 (해당 토큰의 확률, 해당 토큰의 인덱스) 튜플을 반환\n",
    "            top1 = output.max(1)[1]\n",
    "\n",
    "            # Teacher forcing을 사용하면 Ground-truth 토큰을\n",
    "            # Teacher forcing을 사용하지 않는다면, 디코더가 예측한 토큰을 다음 입력 값으로 사용\n",
    "            dec_input = (target[t] if teacher_force else top1)\n",
    "\n",
    "        return outputs, attentions\n",
    "\n",
    "model = Seq2SeqAttention(\n",
    "    input_dim=2000, \n",
    "    embed_dim=200, \n",
    "    enc_hidden_dim=200, \n",
    "    dec_hidden_dim=200, \n",
    "    dropout=0.2,\n",
    "    output_dim=3000, \n",
    "    pad_idx=0\n",
    ")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 조사\n",
    "이제 Pruning이 적용되지 않은 `attnetion` 레이어를 살펴봅시다. 해당 레이어는 현재 `weight`와 `bias` 그리고 `v`를 지니고 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('weight', Parameter containing:\n",
       "  tensor([[ 0.0042,  0.0097,  0.0081,  ..., -0.0141,  0.0324,  0.0173],\n",
       "          [-0.0177, -0.0256, -0.0201,  ..., -0.0254, -0.0268, -0.0381],\n",
       "          [-0.0349,  0.0042, -0.0222,  ...,  0.0366, -0.0176, -0.0316],\n",
       "          ...,\n",
       "          [-0.0004, -0.0259, -0.0041,  ...,  0.0257,  0.0156, -0.0021],\n",
       "          [ 0.0181,  0.0133, -0.0258,  ...,  0.0261,  0.0254, -0.0005],\n",
       "          [-0.0307, -0.0260,  0.0096,  ...,  0.0003, -0.0045,  0.0006]],\n",
       "         requires_grad=True)), ('bias', Parameter containing:\n",
       "  tensor([-0.0395,  0.0304, -0.0336, -0.0144,  0.0356, -0.0219,  0.0299,  0.0010,\n",
       "           0.0209, -0.0233,  0.0165,  0.0103,  0.0139, -0.0338,  0.0012,  0.0333,\n",
       "          -0.0162, -0.0030,  0.0391,  0.0257,  0.0054, -0.0264, -0.0029,  0.0186,\n",
       "           0.0234,  0.0347, -0.0405,  0.0044,  0.0310,  0.0293,  0.0307,  0.0133,\n",
       "          -0.0025,  0.0290, -0.0300,  0.0012,  0.0350, -0.0126,  0.0102, -0.0160,\n",
       "          -0.0335,  0.0241, -0.0059,  0.0091,  0.0217,  0.0389, -0.0003,  0.0115,\n",
       "          -0.0384,  0.0156, -0.0329, -0.0095,  0.0238, -0.0066,  0.0300, -0.0069,\n",
       "           0.0075,  0.0086,  0.0130,  0.0365,  0.0107,  0.0020, -0.0300, -0.0015,\n",
       "           0.0209,  0.0279,  0.0372, -0.0039, -0.0007, -0.0171,  0.0068, -0.0359,\n",
       "           0.0376,  0.0319,  0.0139, -0.0092, -0.0270, -0.0033,  0.0162,  0.0120,\n",
       "          -0.0330, -0.0362, -0.0104, -0.0193,  0.0196,  0.0170, -0.0056, -0.0020,\n",
       "          -0.0400, -0.0007, -0.0402, -0.0094, -0.0315,  0.0070,  0.0347, -0.0346,\n",
       "          -0.0260,  0.0197, -0.0056,  0.0246,  0.0228, -0.0078, -0.0106,  0.0089,\n",
       "          -0.0333,  0.0170, -0.0327, -0.0056, -0.0272,  0.0022,  0.0126,  0.0240,\n",
       "           0.0063,  0.0028, -0.0187, -0.0120, -0.0076,  0.0359, -0.0362,  0.0331,\n",
       "           0.0392, -0.0108,  0.0300,  0.0278, -0.0384,  0.0172,  0.0049, -0.0037,\n",
       "          -0.0090,  0.0007,  0.0340,  0.0167, -0.0149, -0.0159,  0.0291, -0.0017,\n",
       "           0.0113, -0.0182, -0.0287,  0.0122, -0.0093,  0.0030,  0.0344,  0.0140,\n",
       "          -0.0336, -0.0380,  0.0068, -0.0223,  0.0006, -0.0393,  0.0187,  0.0108,\n",
       "           0.0290, -0.0153,  0.0078, -0.0232, -0.0404, -0.0115, -0.0114,  0.0290,\n",
       "          -0.0293,  0.0261,  0.0159,  0.0130, -0.0015, -0.0137,  0.0108, -0.0289,\n",
       "          -0.0374,  0.0182,  0.0155,  0.0219, -0.0139,  0.0113, -0.0257,  0.0100,\n",
       "          -0.0402,  0.0334,  0.0284,  0.0156,  0.0201, -0.0297,  0.0018,  0.0205,\n",
       "          -0.0173, -0.0324,  0.0267, -0.0223, -0.0222,  0.0366,  0.0272, -0.0387,\n",
       "          -0.0069, -0.0089,  0.0055, -0.0322,  0.0202, -0.0237, -0.0325, -0.0085],\n",
       "         requires_grad=True)), ('v', Parameter containing:\n",
       "  tensor([0.0826, 0.1509, 0.5415, 0.1632, 0.7836, 0.2296, 0.8762, 0.9788, 0.8731,\n",
       "          0.0524, 0.8753, 0.1541, 0.9458, 0.0991, 0.0810, 0.5743, 0.1863, 0.8496,\n",
       "          0.8108, 0.5251, 0.3228, 0.9408, 0.5540, 0.5456, 0.2588, 0.0536, 0.3693,\n",
       "          0.8613, 0.8083, 0.5353, 0.5890, 0.3590, 0.9152, 0.4470, 0.7180, 0.9864,\n",
       "          0.5198, 0.1658, 0.6132, 0.9551, 0.5013, 0.1698, 0.3747, 0.2517, 0.4096,\n",
       "          0.2482, 0.0135, 0.3361, 0.0129, 0.0275, 0.6496, 0.6980, 0.6862, 0.6623,\n",
       "          0.1955, 0.4214, 0.1704, 0.6022, 0.5915, 0.7147, 0.2501, 0.2400, 0.3429,\n",
       "          0.5581, 0.2312, 0.9509, 0.6589, 0.9920, 0.1701, 0.5662, 0.3515, 0.6454,\n",
       "          0.8333, 0.5264, 0.5709, 0.0696, 0.6894, 0.3673, 0.3962, 0.1697, 0.0444,\n",
       "          0.6889, 0.4689, 0.8744, 0.2893, 0.1269, 0.0499, 0.7993, 0.2518, 0.6862,\n",
       "          0.1848, 0.7840, 0.9963, 0.8050, 0.0980, 0.9535, 0.3420, 0.1346, 0.5931,\n",
       "          0.8000, 0.7929, 0.4560, 0.8388, 0.7698, 0.7667, 0.6927, 0.0050, 0.8163,\n",
       "          0.7969, 0.6506, 0.7902, 0.2139, 0.8709, 0.5072, 0.1147, 0.3947, 0.9653,\n",
       "          0.6490, 0.4469, 0.6492, 0.6667, 0.6817, 0.0177, 0.0250, 0.1626, 0.2853,\n",
       "          0.3038, 0.1036, 0.1500, 0.7837, 0.7625, 0.4008, 0.5250, 0.6486, 0.9219,\n",
       "          0.9134, 0.6137, 0.6165, 0.3062, 0.2999, 0.0315, 0.7783, 0.7440, 0.5297,\n",
       "          0.6573, 0.9676, 0.7459, 0.0141, 0.0407, 0.1875, 0.5489, 0.5191, 0.5564,\n",
       "          0.2510, 0.4778, 0.0170, 0.2987, 0.6990, 0.7360, 0.6041, 0.4012, 0.8871,\n",
       "          0.4395, 0.7617, 0.5207, 0.6278, 0.0838, 0.9053, 0.2813, 0.6734, 0.8735,\n",
       "          0.0576, 0.6369, 0.8213, 0.1179, 0.1230, 0.2572, 0.3194, 0.1169, 0.0652,\n",
       "          0.4576, 0.7448, 0.1718, 0.3259, 0.1609, 0.0123, 0.5863, 0.3998, 0.6941,\n",
       "          0.8485, 0.5045, 0.8867, 0.7740, 0.6943, 0.8212, 0.4490, 0.6104, 0.3914,\n",
       "          0.3323, 0.6303], requires_grad=True))]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module = model.attention\n",
    "list(module.named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(module.named_buffers())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 Pruning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Attention(\n",
       "  (attention): Linear(in_features=600, out_features=200, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prune.random_unstructured(module, 'v', amount=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('weight', Parameter containing:\n",
       "  tensor([[ 0.0042,  0.0097,  0.0081,  ..., -0.0141,  0.0324,  0.0173],\n",
       "          [-0.0177, -0.0256, -0.0201,  ..., -0.0254, -0.0268, -0.0381],\n",
       "          [-0.0349,  0.0042, -0.0222,  ...,  0.0366, -0.0176, -0.0316],\n",
       "          ...,\n",
       "          [-0.0004, -0.0259, -0.0041,  ...,  0.0257,  0.0156, -0.0021],\n",
       "          [ 0.0181,  0.0133, -0.0258,  ...,  0.0261,  0.0254, -0.0005],\n",
       "          [-0.0307, -0.0260,  0.0096,  ...,  0.0003, -0.0045,  0.0006]],\n",
       "         requires_grad=True)), ('bias', Parameter containing:\n",
       "  tensor([-0.0395,  0.0304, -0.0336, -0.0144,  0.0356, -0.0219,  0.0299,  0.0010,\n",
       "           0.0209, -0.0233,  0.0165,  0.0103,  0.0139, -0.0338,  0.0012,  0.0333,\n",
       "          -0.0162, -0.0030,  0.0391,  0.0257,  0.0054, -0.0264, -0.0029,  0.0186,\n",
       "           0.0234,  0.0347, -0.0405,  0.0044,  0.0310,  0.0293,  0.0307,  0.0133,\n",
       "          -0.0025,  0.0290, -0.0300,  0.0012,  0.0350, -0.0126,  0.0102, -0.0160,\n",
       "          -0.0335,  0.0241, -0.0059,  0.0091,  0.0217,  0.0389, -0.0003,  0.0115,\n",
       "          -0.0384,  0.0156, -0.0329, -0.0095,  0.0238, -0.0066,  0.0300, -0.0069,\n",
       "           0.0075,  0.0086,  0.0130,  0.0365,  0.0107,  0.0020, -0.0300, -0.0015,\n",
       "           0.0209,  0.0279,  0.0372, -0.0039, -0.0007, -0.0171,  0.0068, -0.0359,\n",
       "           0.0376,  0.0319,  0.0139, -0.0092, -0.0270, -0.0033,  0.0162,  0.0120,\n",
       "          -0.0330, -0.0362, -0.0104, -0.0193,  0.0196,  0.0170, -0.0056, -0.0020,\n",
       "          -0.0400, -0.0007, -0.0402, -0.0094, -0.0315,  0.0070,  0.0347, -0.0346,\n",
       "          -0.0260,  0.0197, -0.0056,  0.0246,  0.0228, -0.0078, -0.0106,  0.0089,\n",
       "          -0.0333,  0.0170, -0.0327, -0.0056, -0.0272,  0.0022,  0.0126,  0.0240,\n",
       "           0.0063,  0.0028, -0.0187, -0.0120, -0.0076,  0.0359, -0.0362,  0.0331,\n",
       "           0.0392, -0.0108,  0.0300,  0.0278, -0.0384,  0.0172,  0.0049, -0.0037,\n",
       "          -0.0090,  0.0007,  0.0340,  0.0167, -0.0149, -0.0159,  0.0291, -0.0017,\n",
       "           0.0113, -0.0182, -0.0287,  0.0122, -0.0093,  0.0030,  0.0344,  0.0140,\n",
       "          -0.0336, -0.0380,  0.0068, -0.0223,  0.0006, -0.0393,  0.0187,  0.0108,\n",
       "           0.0290, -0.0153,  0.0078, -0.0232, -0.0404, -0.0115, -0.0114,  0.0290,\n",
       "          -0.0293,  0.0261,  0.0159,  0.0130, -0.0015, -0.0137,  0.0108, -0.0289,\n",
       "          -0.0374,  0.0182,  0.0155,  0.0219, -0.0139,  0.0113, -0.0257,  0.0100,\n",
       "          -0.0402,  0.0334,  0.0284,  0.0156,  0.0201, -0.0297,  0.0018,  0.0205,\n",
       "          -0.0173, -0.0324,  0.0267, -0.0223, -0.0222,  0.0366,  0.0272, -0.0387,\n",
       "          -0.0069, -0.0089,  0.0055, -0.0322,  0.0202, -0.0237, -0.0325, -0.0085],\n",
       "         requires_grad=True)), ('v_orig', Parameter containing:\n",
       "  tensor([0.0826, 0.1509, 0.5415, 0.1632, 0.7836, 0.2296, 0.8762, 0.9788, 0.8731,\n",
       "          0.0524, 0.8753, 0.1541, 0.9458, 0.0991, 0.0810, 0.5743, 0.1863, 0.8496,\n",
       "          0.8108, 0.5251, 0.3228, 0.9408, 0.5540, 0.5456, 0.2588, 0.0536, 0.3693,\n",
       "          0.8613, 0.8083, 0.5353, 0.5890, 0.3590, 0.9152, 0.4470, 0.7180, 0.9864,\n",
       "          0.5198, 0.1658, 0.6132, 0.9551, 0.5013, 0.1698, 0.3747, 0.2517, 0.4096,\n",
       "          0.2482, 0.0135, 0.3361, 0.0129, 0.0275, 0.6496, 0.6980, 0.6862, 0.6623,\n",
       "          0.1955, 0.4214, 0.1704, 0.6022, 0.5915, 0.7147, 0.2501, 0.2400, 0.3429,\n",
       "          0.5581, 0.2312, 0.9509, 0.6589, 0.9920, 0.1701, 0.5662, 0.3515, 0.6454,\n",
       "          0.8333, 0.5264, 0.5709, 0.0696, 0.6894, 0.3673, 0.3962, 0.1697, 0.0444,\n",
       "          0.6889, 0.4689, 0.8744, 0.2893, 0.1269, 0.0499, 0.7993, 0.2518, 0.6862,\n",
       "          0.1848, 0.7840, 0.9963, 0.8050, 0.0980, 0.9535, 0.3420, 0.1346, 0.5931,\n",
       "          0.8000, 0.7929, 0.4560, 0.8388, 0.7698, 0.7667, 0.6927, 0.0050, 0.8163,\n",
       "          0.7969, 0.6506, 0.7902, 0.2139, 0.8709, 0.5072, 0.1147, 0.3947, 0.9653,\n",
       "          0.6490, 0.4469, 0.6492, 0.6667, 0.6817, 0.0177, 0.0250, 0.1626, 0.2853,\n",
       "          0.3038, 0.1036, 0.1500, 0.7837, 0.7625, 0.4008, 0.5250, 0.6486, 0.9219,\n",
       "          0.9134, 0.6137, 0.6165, 0.3062, 0.2999, 0.0315, 0.7783, 0.7440, 0.5297,\n",
       "          0.6573, 0.9676, 0.7459, 0.0141, 0.0407, 0.1875, 0.5489, 0.5191, 0.5564,\n",
       "          0.2510, 0.4778, 0.0170, 0.2987, 0.6990, 0.7360, 0.6041, 0.4012, 0.8871,\n",
       "          0.4395, 0.7617, 0.5207, 0.6278, 0.0838, 0.9053, 0.2813, 0.6734, 0.8735,\n",
       "          0.0576, 0.6369, 0.8213, 0.1179, 0.1230, 0.2572, 0.3194, 0.1169, 0.0652,\n",
       "          0.4576, 0.7448, 0.1718, 0.3259, 0.1609, 0.0123, 0.5863, 0.3998, 0.6941,\n",
       "          0.8485, 0.5045, 0.8867, 0.7740, 0.6943, 0.8212, 0.4490, 0.6104, 0.3914,\n",
       "          0.3323, 0.6303], requires_grad=True))]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(module.named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('v_mask',\n",
       "  tensor([0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1.,\n",
       "          1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0.,\n",
       "          1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1.,\n",
       "          0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1.,\n",
       "          1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1.,\n",
       "          0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
       "          1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1.,\n",
       "          1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1.,\n",
       "          1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
       "          0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
       "          1., 1.]))]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(module.named_buffers())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module.v.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.0000, 0.0000, 0.1632, 0.7836, 0.2296, 0.0000, 0.9788, 0.8731,\n",
       "        0.0000, 0.0000, 0.1541, 0.9458, 0.0991, 0.0810, 0.0000, 0.1863, 0.8496,\n",
       "        0.8108, 0.5251, 0.0000, 0.0000, 0.5540, 0.5456, 0.2588, 0.0536, 0.3693,\n",
       "        0.8613, 0.0000, 0.5353, 0.5890, 0.3590, 0.9152, 0.4470, 0.7180, 0.0000,\n",
       "        0.5198, 0.0000, 0.6132, 0.9551, 0.0000, 0.1698, 0.0000, 0.2517, 0.4096,\n",
       "        0.2482, 0.0135, 0.3361, 0.0000, 0.0275, 0.0000, 0.0000, 0.6862, 0.6623,\n",
       "        0.0000, 0.4214, 0.1704, 0.6022, 0.5915, 0.0000, 0.2501, 0.2400, 0.3429,\n",
       "        0.5581, 0.2312, 0.0000, 0.6589, 0.0000, 0.0000, 0.0000, 0.3515, 0.6454,\n",
       "        0.8333, 0.5264, 0.0000, 0.0000, 0.6894, 0.0000, 0.3962, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.8744, 0.2893, 0.1269, 0.0499, 0.7993, 0.0000, 0.6862,\n",
       "        0.0000, 0.7840, 0.0000, 0.8050, 0.0980, 0.0000, 0.0000, 0.0000, 0.5931,\n",
       "        0.0000, 0.7929, 0.4560, 0.8388, 0.7698, 0.7667, 0.6927, 0.0050, 0.0000,\n",
       "        0.7969, 0.6506, 0.7902, 0.2139, 0.8709, 0.5072, 0.0000, 0.0000, 0.9653,\n",
       "        0.6490, 0.0000, 0.0000, 0.6667, 0.6817, 0.0177, 0.0000, 0.0000, 0.2853,\n",
       "        0.3038, 0.0000, 0.0000, 0.7837, 0.7625, 0.4008, 0.5250, 0.6486, 0.0000,\n",
       "        0.9134, 0.6137, 0.6165, 0.3062, 0.0000, 0.0315, 0.0000, 0.7440, 0.5297,\n",
       "        0.6573, 0.9676, 0.7459, 0.0141, 0.0000, 0.1875, 0.5489, 0.5191, 0.0000,\n",
       "        0.2510, 0.4778, 0.0170, 0.2987, 0.6990, 0.7360, 0.0000, 0.4012, 0.8871,\n",
       "        0.0000, 0.0000, 0.5207, 0.6278, 0.0000, 0.9053, 0.2813, 0.6734, 0.8735,\n",
       "        0.0000, 0.6369, 0.8213, 0.1179, 0.1230, 0.2572, 0.3194, 0.1169, 0.0652,\n",
       "        0.0000, 0.7448, 0.0000, 0.3259, 0.1609, 0.0123, 0.5863, 0.0000, 0.6941,\n",
       "        0.8485, 0.5045, 0.8867, 0.7740, 0.6943, 0.8212, 0.0000, 0.6104, 0.3914,\n",
       "        0.3323, 0.6303], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module.v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(60)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(module.v == 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([(0, <torch.nn.utils.prune.RandomUnstructured at 0x1fa2674b710>)])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module._forward_pre_hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Attention(\n",
       "  (attention): Linear(in_features=600, out_features=200, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prune.l1_unstructured(module, name='bias', amount=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('weight', Parameter containing:\n",
       "  tensor([[ 0.0042,  0.0097,  0.0081,  ..., -0.0141,  0.0324,  0.0173],\n",
       "          [-0.0177, -0.0256, -0.0201,  ..., -0.0254, -0.0268, -0.0381],\n",
       "          [-0.0349,  0.0042, -0.0222,  ...,  0.0366, -0.0176, -0.0316],\n",
       "          ...,\n",
       "          [-0.0004, -0.0259, -0.0041,  ...,  0.0257,  0.0156, -0.0021],\n",
       "          [ 0.0181,  0.0133, -0.0258,  ...,  0.0261,  0.0254, -0.0005],\n",
       "          [-0.0307, -0.0260,  0.0096,  ...,  0.0003, -0.0045,  0.0006]],\n",
       "         requires_grad=True)), ('v_orig', Parameter containing:\n",
       "  tensor([0.0826, 0.1509, 0.5415, 0.1632, 0.7836, 0.2296, 0.8762, 0.9788, 0.8731,\n",
       "          0.0524, 0.8753, 0.1541, 0.9458, 0.0991, 0.0810, 0.5743, 0.1863, 0.8496,\n",
       "          0.8108, 0.5251, 0.3228, 0.9408, 0.5540, 0.5456, 0.2588, 0.0536, 0.3693,\n",
       "          0.8613, 0.8083, 0.5353, 0.5890, 0.3590, 0.9152, 0.4470, 0.7180, 0.9864,\n",
       "          0.5198, 0.1658, 0.6132, 0.9551, 0.5013, 0.1698, 0.3747, 0.2517, 0.4096,\n",
       "          0.2482, 0.0135, 0.3361, 0.0129, 0.0275, 0.6496, 0.6980, 0.6862, 0.6623,\n",
       "          0.1955, 0.4214, 0.1704, 0.6022, 0.5915, 0.7147, 0.2501, 0.2400, 0.3429,\n",
       "          0.5581, 0.2312, 0.9509, 0.6589, 0.9920, 0.1701, 0.5662, 0.3515, 0.6454,\n",
       "          0.8333, 0.5264, 0.5709, 0.0696, 0.6894, 0.3673, 0.3962, 0.1697, 0.0444,\n",
       "          0.6889, 0.4689, 0.8744, 0.2893, 0.1269, 0.0499, 0.7993, 0.2518, 0.6862,\n",
       "          0.1848, 0.7840, 0.9963, 0.8050, 0.0980, 0.9535, 0.3420, 0.1346, 0.5931,\n",
       "          0.8000, 0.7929, 0.4560, 0.8388, 0.7698, 0.7667, 0.6927, 0.0050, 0.8163,\n",
       "          0.7969, 0.6506, 0.7902, 0.2139, 0.8709, 0.5072, 0.1147, 0.3947, 0.9653,\n",
       "          0.6490, 0.4469, 0.6492, 0.6667, 0.6817, 0.0177, 0.0250, 0.1626, 0.2853,\n",
       "          0.3038, 0.1036, 0.1500, 0.7837, 0.7625, 0.4008, 0.5250, 0.6486, 0.9219,\n",
       "          0.9134, 0.6137, 0.6165, 0.3062, 0.2999, 0.0315, 0.7783, 0.7440, 0.5297,\n",
       "          0.6573, 0.9676, 0.7459, 0.0141, 0.0407, 0.1875, 0.5489, 0.5191, 0.5564,\n",
       "          0.2510, 0.4778, 0.0170, 0.2987, 0.6990, 0.7360, 0.6041, 0.4012, 0.8871,\n",
       "          0.4395, 0.7617, 0.5207, 0.6278, 0.0838, 0.9053, 0.2813, 0.6734, 0.8735,\n",
       "          0.0576, 0.6369, 0.8213, 0.1179, 0.1230, 0.2572, 0.3194, 0.1169, 0.0652,\n",
       "          0.4576, 0.7448, 0.1718, 0.3259, 0.1609, 0.0123, 0.5863, 0.3998, 0.6941,\n",
       "          0.8485, 0.5045, 0.8867, 0.7740, 0.6943, 0.8212, 0.4490, 0.6104, 0.3914,\n",
       "          0.3323, 0.6303], requires_grad=True)), ('bias_orig',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0395,  0.0304, -0.0336, -0.0144,  0.0356, -0.0219,  0.0299,  0.0010,\n",
       "           0.0209, -0.0233,  0.0165,  0.0103,  0.0139, -0.0338,  0.0012,  0.0333,\n",
       "          -0.0162, -0.0030,  0.0391,  0.0257,  0.0054, -0.0264, -0.0029,  0.0186,\n",
       "           0.0234,  0.0347, -0.0405,  0.0044,  0.0310,  0.0293,  0.0307,  0.0133,\n",
       "          -0.0025,  0.0290, -0.0300,  0.0012,  0.0350, -0.0126,  0.0102, -0.0160,\n",
       "          -0.0335,  0.0241, -0.0059,  0.0091,  0.0217,  0.0389, -0.0003,  0.0115,\n",
       "          -0.0384,  0.0156, -0.0329, -0.0095,  0.0238, -0.0066,  0.0300, -0.0069,\n",
       "           0.0075,  0.0086,  0.0130,  0.0365,  0.0107,  0.0020, -0.0300, -0.0015,\n",
       "           0.0209,  0.0279,  0.0372, -0.0039, -0.0007, -0.0171,  0.0068, -0.0359,\n",
       "           0.0376,  0.0319,  0.0139, -0.0092, -0.0270, -0.0033,  0.0162,  0.0120,\n",
       "          -0.0330, -0.0362, -0.0104, -0.0193,  0.0196,  0.0170, -0.0056, -0.0020,\n",
       "          -0.0400, -0.0007, -0.0402, -0.0094, -0.0315,  0.0070,  0.0347, -0.0346,\n",
       "          -0.0260,  0.0197, -0.0056,  0.0246,  0.0228, -0.0078, -0.0106,  0.0089,\n",
       "          -0.0333,  0.0170, -0.0327, -0.0056, -0.0272,  0.0022,  0.0126,  0.0240,\n",
       "           0.0063,  0.0028, -0.0187, -0.0120, -0.0076,  0.0359, -0.0362,  0.0331,\n",
       "           0.0392, -0.0108,  0.0300,  0.0278, -0.0384,  0.0172,  0.0049, -0.0037,\n",
       "          -0.0090,  0.0007,  0.0340,  0.0167, -0.0149, -0.0159,  0.0291, -0.0017,\n",
       "           0.0113, -0.0182, -0.0287,  0.0122, -0.0093,  0.0030,  0.0344,  0.0140,\n",
       "          -0.0336, -0.0380,  0.0068, -0.0223,  0.0006, -0.0393,  0.0187,  0.0108,\n",
       "           0.0290, -0.0153,  0.0078, -0.0232, -0.0404, -0.0115, -0.0114,  0.0290,\n",
       "          -0.0293,  0.0261,  0.0159,  0.0130, -0.0015, -0.0137,  0.0108, -0.0289,\n",
       "          -0.0374,  0.0182,  0.0155,  0.0219, -0.0139,  0.0113, -0.0257,  0.0100,\n",
       "          -0.0402,  0.0334,  0.0284,  0.0156,  0.0201, -0.0297,  0.0018,  0.0205,\n",
       "          -0.0173, -0.0324,  0.0267, -0.0223, -0.0222,  0.0366,  0.0272, -0.0387,\n",
       "          -0.0069, -0.0089,  0.0055, -0.0322,  0.0202, -0.0237, -0.0325, -0.0085],\n",
       "         requires_grad=True))]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(module.named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0395,  0.0304, -0.0336, -0.0144,  0.0356, -0.0219,  0.0299,  0.0010,\n",
       "         0.0209, -0.0233,  0.0165,  0.0103,  0.0139, -0.0338,  0.0012,  0.0333,\n",
       "        -0.0162, -0.0030,  0.0391,  0.0257,  0.0054, -0.0264, -0.0029,  0.0186,\n",
       "         0.0234,  0.0347, -0.0405,  0.0044,  0.0310,  0.0293,  0.0307,  0.0133,\n",
       "        -0.0025,  0.0290, -0.0300,  0.0012,  0.0350, -0.0126,  0.0102, -0.0160,\n",
       "        -0.0335,  0.0241, -0.0059,  0.0091,  0.0217,  0.0389, -0.0000,  0.0115,\n",
       "        -0.0384,  0.0156, -0.0329, -0.0095,  0.0238, -0.0066,  0.0300, -0.0069,\n",
       "         0.0075,  0.0086,  0.0130,  0.0365,  0.0107,  0.0020, -0.0300, -0.0015,\n",
       "         0.0209,  0.0279,  0.0372, -0.0039, -0.0000, -0.0171,  0.0068, -0.0359,\n",
       "         0.0376,  0.0319,  0.0139, -0.0092, -0.0270, -0.0033,  0.0162,  0.0120,\n",
       "        -0.0330, -0.0362, -0.0104, -0.0193,  0.0196,  0.0170, -0.0056, -0.0020,\n",
       "        -0.0400, -0.0007, -0.0402, -0.0094, -0.0315,  0.0070,  0.0347, -0.0346,\n",
       "        -0.0260,  0.0197, -0.0056,  0.0246,  0.0228, -0.0078, -0.0106,  0.0089,\n",
       "        -0.0333,  0.0170, -0.0327, -0.0056, -0.0272,  0.0022,  0.0126,  0.0240,\n",
       "         0.0063,  0.0028, -0.0187, -0.0120, -0.0076,  0.0359, -0.0362,  0.0331,\n",
       "         0.0392, -0.0108,  0.0300,  0.0278, -0.0384,  0.0172,  0.0049, -0.0037,\n",
       "        -0.0090,  0.0007,  0.0340,  0.0167, -0.0149, -0.0159,  0.0291, -0.0017,\n",
       "         0.0113, -0.0182, -0.0287,  0.0122, -0.0093,  0.0030,  0.0344,  0.0140,\n",
       "        -0.0336, -0.0380,  0.0068, -0.0223,  0.0000, -0.0393,  0.0187,  0.0108,\n",
       "         0.0290, -0.0153,  0.0078, -0.0232, -0.0404, -0.0115, -0.0114,  0.0290,\n",
       "        -0.0293,  0.0261,  0.0159,  0.0130, -0.0015, -0.0137,  0.0108, -0.0289,\n",
       "        -0.0374,  0.0182,  0.0155,  0.0219, -0.0139,  0.0113, -0.0257,  0.0100,\n",
       "        -0.0402,  0.0334,  0.0284,  0.0156,  0.0201, -0.0297,  0.0018,  0.0205,\n",
       "        -0.0173, -0.0324,  0.0267, -0.0223, -0.0222,  0.0366,  0.0272, -0.0387,\n",
       "        -0.0069, -0.0089,  0.0055, -0.0322,  0.0202, -0.0237, -0.0325, -0.0085],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('v_mask',\n",
       "  tensor([0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1.,\n",
       "          1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0.,\n",
       "          1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1.,\n",
       "          0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1.,\n",
       "          1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1.,\n",
       "          0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
       "          1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1.,\n",
       "          1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1.,\n",
       "          1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
       "          0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
       "          1., 1.])),\n",
       " ('bias_mask',\n",
       "  tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1.]))]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(module.named_buffers())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned = (list(module.named_buffers())[-1][-1] == 0).nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([46])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pruned[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0., grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module.bias[46]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.0003, grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(module.named_parameters())[-1][-1][46]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([(0, <torch.nn.utils.prune.RandomUnstructured at 0x1fa2674b710>),\n",
       "             (1, <torch.nn.utils.prune.L1Unstructured at 0x1fa2679c320>)])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module._forward_pre_hooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 반복적인 Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

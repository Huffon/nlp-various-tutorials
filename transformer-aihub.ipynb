{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Annotated Transformer 보다 친절한 트랜스포머 튜토리얼 !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. 데이터 가공\n",
    "- 먼저, [**AI Hub**](http://www.aihub.or.kr/)에서 [**한국어-영어 번역 말뭉치**](http://www.aihub.or.kr/aidata/87)를 요청해야 합니다.\n",
    "- 신청 후, 약 **2일** 내에 승인 결과가 메일로 전달된다고 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('data/corpus.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한국어, 영어 데이터 별개로 저장\n",
    "\n",
    "kor_lines = []\n",
    "eng_lines = []\n",
    "\n",
    "for _, row in data.iterrows():\n",
    "    _, kor, eng = row\n",
    "    kor_lines.append(kor)\n",
    "    eng_lines.append(eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[KOR]: 나는 매일 저녁 배트를 만나러 다락방으로 가요.\n",
      "[ENG]: I go to the attic every evening to meet Bat.\n",
      "\n",
      "[KOR]: 선생님 이문장이 이해가 안 가요.\n",
      "[ENG]: Sir, I don't understand this sentence here.\n",
      "\n",
      "[KOR]: 컴퓨터를 시작하면 시간이 너무 빠르게 가요.\n",
      "[ENG]: Time flies when you start using the computer.\n",
      "\n",
      "[KOR]: 나는 오늘 자정에 한국으로 돌아 가요.\n",
      "[ENG]: I'm going back to Korea today at midnight.\n",
      "\n",
      "[KOR]: 나는 일어나자마자 화장실에 가요.\n",
      "[ENG]: I go to bathroom as soon as I wake up.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for kor, eng in zip(kor_lines[:5], eng_lines[:5]):\n",
    "    print(f'[KOR]: {kor}')\n",
    "    print(f'[ENG]: {eng}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_korean.txt', 'w', encoding='utf-8') as f:\n",
    "    for line in kor_lines:\n",
    "        print(line, file=f)\n",
    "        \n",
    "with open('train_english.txt', 'w', encoding='utf-8') as f:\n",
    "    for line in eng_lines:\n",
    "        print(line, file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. BPE 토크나이저 학습\n",
    "- 앞서 가공한 데이터를 활용해 **BPE 토크나이저**를 학습시킵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'batch_size': 64,\n",
    "    'num_epoch': 15,\n",
    "    'dropout': 0.1,\n",
    "    'min_frequency': 3,\n",
    "    'max_len': 512,\n",
    "    \n",
    "    'vocab_size': 20000,\n",
    "    'num_layers': 6,\n",
    "    'num_heads': 8,\n",
    "    'hidden_dim': 512,\n",
    "    'ffn_dim': 2048,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import BPETokenizer\n",
    "\n",
    "# 한국어 토크나이저 초기화\n",
    "\n",
    "kor_tokenizer = BPETokenizer()\n",
    "\n",
    "\n",
    "# 한국어 토크나이저 훈련\n",
    "\n",
    "kor_tokenizer.train(\n",
    "    ['train_korean.txt'],\n",
    "    vocab_size=params['vocab_size'],\n",
    "    min_frequency=params['min_frequency'],\n",
    "    special_tokens=['[PAD]', '[SOS]', '[EOS]', '[UNK]'],\n",
    "    suffix=''\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 영어 토크나이저 초기화\n",
    "\n",
    "eng_tokenizer = BPETokenizer()\n",
    "\n",
    "\n",
    "# 영어 토크나이저 훈련\n",
    "\n",
    "eng_tokenizer.train(\n",
    "    ['train_english.txt'],\n",
    "    vocab_size=params['vocab_size'],\n",
    "    min_frequency=params['min_frequency'],\n",
    "    special_tokens=['[PAD]', '[SOS]', '[EOS]', '[UNK]'],\n",
    "    suffix=''\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'BPE', 'unk_token': '<unk>', 'suffix': '</w>', 'dropout': None}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kor_tokenizer._parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_tokenizer._parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_idx = kor_tokenizer.token_to_id('[PAD]')\n",
    "sos_idx = kor_tokenizer.token_to_id('[SOS]')\n",
    "eos_idx = kor_tokenizer.token_to_id('[EOS]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "kor_tokenizer.enable_padding(pad_id=pad_idx, pad_token='[PAD]', max_length=params['max_len'])\n",
    "eng_tokenizer.enable_padding(pad_id=pad_idx, pad_token='[PAD]', max_length=params['max_len'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "kor_encoded_data = kor_tokenizer.encode_batch(kor_lines)\n",
    "eng_encoded_data = eng_tokenizer.encode_batch(eng_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(input_ids):\n",
    "    input_ids = [sos_idx] + input_ids\n",
    "    \n",
    "    input_ids = input_ids[:params['max_len']]\n",
    "\n",
    "    if pad_idx in input_ids:\n",
    "        pad_start = input_ids.index(pad_idx)\n",
    "        input_ids[pad_start] = eos_idx\n",
    "    else:\n",
    "        input_ids[-1] = eos_idx\n",
    "    \n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "kor_processed_data = [postprocess(data.ids) for data in kor_encoded_data]\n",
    "eng_processed_data = [postprocess(data.ids) for data in eng_encoded_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "random.seed(32)\n",
    "np.random.seed(32)\n",
    "torch.manual_seed(32)\n",
    "torch.cuda.manual_seed(32)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "from torch.utils import data\n",
    "\n",
    "kor_tensors = [torch.LongTensor(line).to(device) for line in kor_processed_data]\n",
    "eng_tensors = [torch.LongTensor(line).to(device) for line in eng_processed_data]\n",
    "\n",
    "src_iter = data.DataLoader(kor_tensors, batch_size=params['batch_size'], shuffle=True, worker_init_fn=np.random.seed(12))\n",
    "tgt_iter = data.DataLoader(eng_tensors, batch_size=params['batch_size'], shuffle=True, worker_init_fn=np.random.seed(12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: 몇시에도착할건지말해줄래요?\n",
      "Target: inthattime,damagedcasewasalreadyexcluded.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for src, tgt in zip(src_iter, tgt_iter):\n",
    "    src = src.detach().cpu().numpy()\n",
    "    tgt = tgt.detach().cpu().numpy()\n",
    "    print(f'Source: {kor_tokenizer.decode(src[0])}')\n",
    "    print(f'Target: {eng_tokenizer.decode(tgt[0])}\\n')\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Transformer 모델 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 먼저 모델 구현에 필요한 라이브러리들을 모두 임포트합니다.\n",
    "- 실험을 함에 있어 항상 실험의 **Reproducibility**를 보장하기 위해 Seed 설정을 해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(32)\n",
    "torch.cuda.manual_seed(32)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    '''멀티 헤드 어텐션 레이어'''\n",
    "    def __init__(self, params):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert params['hidden_dim'] % params['num_heads'] == 0, \"hidden dimension must be divisible by the number of heads\"\n",
    "        self.num_heads = params['num_heads']\n",
    "        self.attn_dim = params['hidden_dim'] // self.num_heads\n",
    "        \n",
    "        self.q_w = nn.Linear(params['hidden_dim'], self.num_heads * self.attn_dim)\n",
    "        self.k_w = nn.Linear(params['hidden_dim'], self.num_heads * self.attn_dim)\n",
    "        self.v_w = nn.Linear(params['hidden_dim'], self.num_heads * self.attn_dim)\n",
    "        \n",
    "        self.o_w = nn.Linear(self.num_heads * self.attn_dim, params['hidden_dim'])\n",
    "        \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        \" q, k, v = [배치 사이즈, 문장 길이, 은닉 차원] \"\n",
    "        \n",
    "        batch_size = q.size(0)\n",
    "        \n",
    "        q = self.q_w(q).view(batch_size, -1, self.num_heads, self.attn_dim).transpose(1, 2)\n",
    "        k = self.k_w(k).view(batch_size, -1, self.num_heads, self.attn_dim).transpose(1, 2)\n",
    "        v = self.v_w(v).view(batch_size, -1, self.num_heads, self.attn_dim).transpose(1, 2)\n",
    "        # q, k, v = [배치 사이즈, 헤드 갯수, 문장 길이, 어텐션 차원]\n",
    "        \n",
    "        attn = torch.matmul(q, k.transpose(-1, -2))\n",
    "        # attn = [배치 사이즈, 헤드 갯수, 문장 길이, 문장 길이]\n",
    "        \n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "            attn.masked_fill(mask==0, -1e9)\n",
    "        \n",
    "        score = F.softmax(attn, dim=-1)\n",
    "        # score = [배치 사이즈, 헤드 갯수, 문장 길이, 문장 길이]\n",
    "        \n",
    "        output = torch.matmul(score, v)\n",
    "        # output = [배치 사이즈, 헤드 갯수, 문장 길이, 어텐션 차원]\n",
    "        \n",
    "        output = output.transpose(1, 2).contiguous()\n",
    "        # output = [배치 사이즈, 문장 길이, 헤드 갯수, 어텐션 차원]\n",
    "        \n",
    "        output = output.view(batch_size, -1, self.num_heads * self.attn_dim)\n",
    "        # output = [배치 사이즈, 문장 길이, 은닉 차원]\n",
    "        \n",
    "        output = self.o_w(output)\n",
    "        # output = [배치 사이즈, 문장 길이, 은닉 차원]\n",
    "        \n",
    "        return output, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_src_mask(src):\n",
    "    \" source = [배치 사이즈, 소스 문장 길이] \"\n",
    "\n",
    "    src_len = src.size(1)\n",
    "    \n",
    "    src_mask = (src == pad_idx)\n",
    "    # src_mask = [배치 사이즈, 소스 문장 길이]\n",
    "    \n",
    "    src_mask = src_mask.unsqueeze(1).repeat(1, src_len, 1)\n",
    "    # src_mask = [배치 사이즈, 소스 문장 길이, 소스 문장 길이]\n",
    "    \n",
    "    return src_mask\n",
    "\n",
    "\n",
    "def create_tgt_mask(src, tgt):\n",
    "    \" src = [배치 사이즈, 소스 문장 길이] \"\n",
    "    \" tgt = [배치 사이즈, 타겟 문장 길이] \"\n",
    "\n",
    "    batch_size, tgt_len = tgt.size()\n",
    "    \n",
    "    subsequent_mask = torch.triu(torch.ones(tgt_len, tgt_len), diagonal=1)\n",
    "    # subsequent_mask = [타겟 문장 길이, 타겟 문장 길이]\n",
    "    \n",
    "    subsequent_mask = subsequent_mask.unsqueeze(0).repeat(batch_size, 1, 1)\n",
    "    # subsquent_mask = [배치 사이즈, 타겟 문장 길이, 타겟 문장 길이]\n",
    "    \n",
    "    src_mask = (src == pad_idx)\n",
    "    tgt_mask = (tgt == pad_idx)\n",
    "    # src_mask = [배치 사이즈, 소스 문장 길이]\n",
    "    # tgt_mask = [배치 사이즈, 타겟 문장 길이]\n",
    "    \n",
    "    src_mask = src_mask.unsqueeze(1).repeat(1, tgt_len, 1)\n",
    "    tgt_mask = tgt_mask.unsqueeze(1).repeat(1, tgt_len, 1)\n",
    "    # src_mask = [배치 사이즈, 타겟 문장 길이, 소스 문장 길이]\n",
    "    # tgt_mask = [배치 사이즈, 타겟 문장 길이, 타겟 문장 길이]\n",
    "    \n",
    "    tgt_mask = target_mask | subsequent_mask\n",
    "    \n",
    "    return src_mask, tgt_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/positionwise.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    '''포지션 와이즈 피드 포워드 레이어'''\n",
    "    def __init__(self, parmas):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(params['hidden_dim'], params['ffn_dim'])\n",
    "        self.fc2 = nn.Linear(params['ffn_dim'], params['hidden_dim'])\n",
    "        self.dropout = nn.Dropout(params['dropout'])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \" x = [배치 사이즈, 문장 길이, 은닉 차원] \"\n",
    "\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/pos.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        sinusoid = np.array([pos / np.power(10000, 2 * i / params['hidden_dim'])\n",
    "                            for pos in range(params['max_len']) for i in range(params['hidden_dim'])])\n",
    "        # sinusoid = [문장 최대 길이 * 은닉 차원]\n",
    "\n",
    "        sinusoid = sinusoid.reshape(params['max_len'], -1)\n",
    "        # sinusoid = [문장 최대 길이, 은닉 차원]\n",
    "\n",
    "        sinusoid[:, 0::2] = np.sin(sinusoid[:, 0::2])\n",
    "        sinusoid[:, 1::2] = np.cos(sinusoid[:, 1::2])\n",
    "        sinusoid = torch.FloatTensor(sinusoid).to(device)\n",
    "\n",
    "        self.embedding = nn.Embedding.from_pretrained(sinusoid, freeze=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \" x = [배치 사이즈, 문장 길이] \"\n",
    "        \n",
    "        pos = torch.arange(x.size(-1), dtype=torch.long).to(device)\n",
    "        # pos = [배치 사이즈, 문장 길이]\n",
    "\n",
    "        embed = self.embedding(pos)\n",
    "        # embed = [배치 사이즈, 문장 길이, 은닉 차원]\n",
    "        return embed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/encoder.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    '''인코더 레이어'''\n",
    "    def __init__(self, params):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(params)\n",
    "        self.layer_norm1 = nn.LayerNorm(params['hidden_dim'])\n",
    "        self.feed_forward = PositionwiseFeedForward(params)\n",
    "        self.layer_norm2 = nn.LayerNorm(params['hidden_dim'])\n",
    "        self.dropout = nn.Dropout(params['dropout'])\n",
    "        \n",
    "    def forward(self, x, src_mask):\n",
    "        \" x = [배치 사이즈, 문장 길이, 은닉 차원] \"\n",
    "        \n",
    "        residual = x\n",
    "        x = self.self_attn(x, x, x, src_mask)\n",
    "        x = self.dropout(x)\n",
    "        x = residual + x\n",
    "        x = self.layer_norm1(x)\n",
    "        \n",
    "        residual = x\n",
    "        x = self.feed_forward(x)\n",
    "        x = self.dropout(x)\n",
    "        x = resiudal + x\n",
    "        x = self.layer_norm2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    '''트랜스포머 인코더'''\n",
    "    def __init__(self, params):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.tok_embedding = nn.Embedding(params['vocab_size'], params['hidden_dim'], padding_idx=pad_idx)\n",
    "        self.pos_embedding = PositionalEncoding(params)\n",
    "        self.layers = nn.ModuleList([EncoderLayer(params) for _ in range(params['num_layers'])])\n",
    "        \n",
    "    def forward(self, src):\n",
    "        \" src = [배치 사이즈, 소스 문장 길이] \"\n",
    "\n",
    "        src_mask = create_src_mask(src)\n",
    "        src = self.tok_embedding(src) + self.pos_embedding(src)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            src = layer(src, src_mask)\n",
    "            \n",
    "        # src = [배치 사이즈, 소스 문장 길이, 은닉 차원]\n",
    "        return src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/decoder.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    '''디코더 레이어'''\n",
    "    def __init__(self, params):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(params)\n",
    "        self.layer_norm1 = nn.LayerNorm(params['hidden_dim'])\n",
    "\n",
    "        self.enc_dec_attn = MultiHeadAttention(params)\n",
    "        self.layer_norm2 = nn.LayerNorm(params['hidden_dim'])\n",
    "        \n",
    "        self.feed_forward = PositionwiseFeedForward(params)\n",
    "        self.layer_norm3 = nn.LayerNorm(params['hidden_dim'])\n",
    "        \n",
    "        self.dropout = nn.Dropout(params['dropout'])\n",
    "        \n",
    "    def forward(self, x, tgt_mask, enc_output, src_mask):\n",
    "        \" x = [배치 사이즈, 문장 길이, 은닉 차원] \"\n",
    "        \n",
    "        residual = x\n",
    "        x = self.self_attn(x, x, x, tgt_mask)\n",
    "        x = self.dropout(x)\n",
    "        x = residual + x\n",
    "        x = self.layer_norm1(x)\n",
    "        \n",
    "        residual = x\n",
    "        x = self.enc_dec_attn(x, enc_output, enc_output, src_mask)\n",
    "        x = self.dropout(x)\n",
    "        x = residual + x\n",
    "        x = self.layer_norm2(x)\n",
    "        \n",
    "        residual = x\n",
    "        x = self.feed_forward(x)\n",
    "        x = self.dropout(x)\n",
    "        x = resiudal + x\n",
    "        x = self.layer_norm3(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    '''트랜스포머 디코더'''\n",
    "    def __init__(self, params):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.tok_embedding = nn.Embedding(params['vocab_size'], params['hidden_dim'], padding_idx=pad_idx)\n",
    "        self.pos_embedding = PositionalEncoding(params)\n",
    "        self.layers = nn.ModuleList([DecoderLayer(params) for _ in range(params['num_layers'])])\n",
    "        \n",
    "    def forward(self, tgt, enc_out):\n",
    "        \" tgt = [배치 사이즈, 타겟 문장 길이] \"\n",
    "\n",
    "        src_mask, tgt_mask = create_tgt_mask(enc_out, tgt)\n",
    "        tgt = self.tok_embedding(tgt) + self.pos_embedding(tgt)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            tgt, attn_map = layer(tgt, tgt_mask, enc_out, src_mask)\n",
    "            \n",
    "        tgt = torch.matmul(tgt, self.tok_embedding.weight.transpose(0, 1))\n",
    "        # tgt = [배치 사이즈, 타겟 문장 길이, 은닉 차원]\n",
    "\n",
    "        return tgt, attn_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    '''트랜스포머 네트워크'''\n",
    "    def __init__(self, params):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = Encoder(params)\n",
    "        self.decoder = Decoder(params)\n",
    "    \n",
    "    def forward(self, src, tgt):\n",
    "        \" src = [배치 사이즈, 소스 문장 길이] \"\n",
    "        \" tgt = [배치 사이즈, 타겟 문장 길이] \"\n",
    "        \n",
    "        enc_out = self.encoder(src)\n",
    "        dec_out, attn = self.decoder(tgt, enc_out)\n",
    "        return dec_out, attn\n",
    "    \n",
    "    def count_params(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/optim.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScheduledOptim:\n",
    "    '''스케줄 옵티마이저'''\n",
    "    def __init__(self, optimizer, warmup_steps):\n",
    "        self.init_lr = np.power(params['hidden_dim'], -0.5)\n",
    "        self.optimizer = optimizer\n",
    "        self.step_num = 0\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def step(self):\n",
    "        self.step_num += 1\n",
    "        lr = self.init_lr * self.get_scale()\n",
    "        \n",
    "        for p in self.optimizer.param_gropus:\n",
    "            p['lr'] = lr\n",
    "            \n",
    "        self.optimizer.step()\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        self.optimizer.zero_grad()\n",
    "    \n",
    "    def get_scale(self):\n",
    "        return np.min([\n",
    "            np.power(self.step_num, -0.5),\n",
    "            self.step_num * np.power(self.warmup_steps, -1.5)\n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 8, 512, 512])\n",
      "torch.Size([64, 512, 512])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 512.00 MiB (GPU 0; 6.00 GiB total capacity; 4.32 GiB already allocated; 369.14 MiB free; 12.56 MiB cached)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-176-1491f11925dd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtgt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m         \u001b[1;31m# logits = [배치 사이즈, 타겟 문장 길이, 은닉 차원]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-174-77e5fbf21135>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, src, tgt)\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;34m\" tgt = [배치 사이즈, 타겟 문장 길이] \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0menc_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[0mdec_out\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtgt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menc_out\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdec_out\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-172-3eb512fed332>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, src)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0msrc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[1;31m# src = [배치 사이즈, 소스 문장 길이, 은닉 차원]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-172-3eb512fed332>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, src_mask)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mself_attn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresidual\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-168-b81cefb72d18>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, q, k, v, mask)\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmask\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m             \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m             \u001b[0mattn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmasked_fill\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1e9\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 512.00 MiB (GPU 0; 6.00 GiB total capacity; 4.32 GiB already allocated; 369.14 MiB free; 12.56 MiB cached)"
     ]
    }
   ],
   "source": [
    "model = Transformer(params)\n",
    "model.to(device)\n",
    "model.count_params()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "criterion.to(device)\n",
    "\n",
    "optimizer = ScheduledOptim(\n",
    "    optim.Adam(model.parameters(), betas=[0.9, 0.98], eps=1e-9),\n",
    "    warmup_steps=4000\n",
    ")\n",
    "\n",
    "for epoch in range(params['num_epoch']):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for src, tgt in zip(src_iter, tgt_iter):\n",
    "        \" src = [배치 사이즈, 소스 문장 길이] \"\n",
    "        \" tgt = [배치 사이즈, 타겟 문장 길이] \"\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        logits, _ = model(src, tgt[:, :-1])\n",
    "        # logits = [배치 사이즈, 타겟 문장 길이, 은닉 차원]\n",
    "        \n",
    "        logits = logits.contiguous().view(-1, logits.size(-1))\n",
    "        # logits = [(배치 사이즈 * 타겟 문장 길이) - 1, 은닉 차원]\n",
    "        golds = target[:, 1:].contiguous.view(-1)\n",
    "        # golds = [(배치 사이즈 * 타겟 문장 길이) - 1]\n",
    "\n",
    "        loss = criterion(logits, golds)\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "#         torch.nn.utils.clip_grad_norm_(model.parameters(), self.params.clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 결과 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict():\n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 참고자료\n",
    "- [The Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html)\n",
    "- [jadore801120/attention-is-all-you-need-pytorch](https://github.com/jadore801120/attention-is-all-you-need-pytorch)\n",
    "- [tunz/transformer-pytorch](https://github.com/tunz/transformer-pytorch)\n",
    "- [IgorSusmelj/pytorch-styleguide](https://github.com/IgorSusmelj/pytorch-styleguide)\n",
    "\n",
    "\n",
    "### TODO\n",
    "- **Beam Search** 디코딩 추가\n",
    "- **Label Smoothing** 기법 추가"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
